{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.5.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "import platform\n",
    "import random\n",
    "import pickle as pickle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def num_datapoints(filename):\n",
    "    \"\"\"\n",
    "    Given the name of a CVS file, the function returns the number of rows of data.\n",
    "    Inputs:\n",
    "    - filename: name of CVS file\n",
    "    Returns:\n",
    "    The number of rows of data\n",
    "    \"\"\"\n",
    "    \n",
    "    row_count = 0\n",
    "    \n",
    "    f = open(filename, 'rt')\n",
    "    for line in f:\n",
    "        row_count += 1\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    return row_count\n",
    "\n",
    "def num_chars(filename):\n",
    "    \"\"\"\n",
    "    Given the name of a CVS file, the function returns the number of characters in the file.\n",
    "    Inputs:\n",
    "    - filename: name of CVS file\n",
    "    Returns:\n",
    "    The number of rows of data\n",
    "    \"\"\"\n",
    "    \n",
    "    char_count = 0\n",
    "    \n",
    "    f = open(filename, 'rt')\n",
    "    for line in f:\n",
    "        for c in line:\n",
    "            char_count += 1\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    return char_count\n",
    "\n",
    "\n",
    "def extract_characters(filename):\n",
    "    \"\"\"\n",
    "    Given the name of a file, the function returns a list of unique characters in the file.\n",
    "    Inputs:\n",
    "    - filename: name of CVS file\n",
    "    Returns:\n",
    "    A list of unique character\n",
    "    \"\"\"\n",
    "    characters = []\n",
    "    f = open(filename, 'rt')\n",
    "\n",
    "    for line in f:\n",
    "        for c in line:\n",
    "            if c not in characters:\n",
    "                characters.append(c)\n",
    "    f.close()        \n",
    "    return characters\n",
    "\n",
    "\n",
    "def letterToIndex(letter, char_dict):\n",
    "    \"\"\"\n",
    "    Find letter index from all_letters, e.g. \"\\t\" = 0\n",
    "    Inputs:\n",
    "    - letter\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    Index of the letter in the dictionary\n",
    "    \"\"\"\n",
    "    return char_dict.index(letter)\n",
    "\n",
    "def letterToTensor(letter, char_dict):\n",
    "    \"\"\"\n",
    "    Turn a letter into a <1 x len(char_dict)> Tensor representing its one-hot encoding.\n",
    "    Inputs:\n",
    "    - letter\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    tensor: One-hot encoding of the letter\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(1, len(char_dict))\n",
    "    tensor[0, letterToIndex(letter, char_dict)] = 1\n",
    "    return tensor.view(1,-1)\n",
    "\n",
    "\n",
    "def tensorToLetter(tensor, char_dict):\n",
    "    \"\"\"\n",
    "    Turn a one-hot <1 x len(char_dict)> Tensor back to the letter\n",
    "    Inputs:\n",
    "    - letter\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    letter: One-hot encoding of the letter\n",
    "    \"\"\"\n",
    "    index = torch.max(tensor.view(1,-1),1)[1]   # force tensor to 1xN dimension\n",
    "    return char_dict[index.numpy()[0]]\n",
    "\n",
    "\n",
    "def lineToTensor(line, char_dict):\n",
    "    \"\"\"\n",
    "    Turn a line into an array of one-hot tensors <line_length x 1 x len(char_dict)>\n",
    "    Inputs:\n",
    "    - line\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    tensor: an array of one-hot tensors\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(line), len(char_dict))\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li, letterToIndex(letter, char_dict)] = 1\n",
    "    return tensor\n",
    "\n",
    "def display_batch(minibatch, char_dict):\n",
    "    line = ''\n",
    "    for tensor in minibatch:\n",
    "        char = tensorToLetter(tensor, char_dict)\n",
    "        line = line + char\n",
    "    return line\n",
    "\n",
    "def generateCorpus(filename, dictionary):\n",
    "    \"\"\"\n",
    "    Take all the content in a text file and generate a corpus of one-hot encodings.\n",
    "    Inputs:\n",
    "    - filename\n",
    "    - dictionary: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    corpus: a tensor of <total_chars, 1, len(dictionary)>\n",
    "    \"\"\"\n",
    "    \n",
    "    total_chars = num_chars(filename)  # figure out how many char in the file\n",
    "    corpus = torch.zeros(total_chars, len(dictionary))\n",
    "    i = 0\n",
    "\n",
    "    f = open(filename, 'rt')\n",
    "    for line in f:\n",
    "        for c in line:\n",
    "            corpus[i, letterToIndex(c, dictionary)] = 1  # one-hot encoding\n",
    "            i += 1\n",
    "            \n",
    "    f.close()  \n",
    "    return corpus\n",
    "\n",
    "def prepare_sequence(batch):\n",
    "    seq = []\n",
    "    for i in range(batch.size([0])):\n",
    "        seq.append(batch[i,:])\n",
    "        tensor = torch.LongTensor(seq)\n",
    "    return Variable(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "\n",
      "X:1\n",
      "\n",
      "T: La Montfarine\n",
      "\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-07-24\n",
      "\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\n",
      "\n",
      "M: 4/4\n",
      "\n",
      "L: 1/8\n",
      "\n",
      "Q:1/4=186\n",
      "\n",
      "FGF B=AG G=AG F2F FGF {F}F2E EFE|\n",
      "\n",
      "{E}E2D FGF B=AG G=AG {F}F2F FED C2G D2E|F3 {F}F/2 ED E3/2D/2|\n",
      "\n",
      "<class 'str'>\n",
      "18711\n",
      "501470\n",
      "['\\t', '\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "filename = 'input.txt'\n",
    "\n",
    "inputfile = open(filename, 'rt')\n",
    "\n",
    "# explore the content of the input.txt file\n",
    "i = 0\n",
    "for line in inputfile:\n",
    "    if i < 10:\n",
    "        print (line)\n",
    "        i += 1\n",
    "    \n",
    "inputfile.close()\n",
    "\n",
    "print (type(line))  # line are read in as strings\n",
    "\n",
    "print (num_datapoints(filename))  # 18711 rows in the file\n",
    "print (num_chars(filename))  # 501470 characters in the file\n",
    "\n",
    "char_dict = sorted(extract_characters(filename))   # 93 unique characters\n",
    "\n",
    "print (char_dict)\n",
    "print (len(char_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([501470, 93])\n",
      "<start>\n",
      "X:1\n",
      "T: La Montfarine\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-07-24\n",
      "Z:Pour toute o\n"
     ]
    }
   ],
   "source": [
    "filename = 'input.txt'\n",
    "corpus = generateCorpus(filename, char_dict)\n",
    "\n",
    "print (corpus.size())\n",
    "print(display_batch(corpus[0:100,:], char_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 401470\n",
    "NUM_VAL = 100000\n",
    "\n",
    "loader_train = DataLoader(corpus, batch_size=32, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "loader_val = DataLoader(corpus, batch_size=32, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_dtype = torch.FloatTensor # the CPU datatype\n",
    "gpu_dtype = torch.cuda.FloatTensor  # the GPU datatype\n",
    "\n",
    "class LSTM_ABC(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTM_ABC, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # We encode the character as 1-hot, so skip this step\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes 1-hot as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to output (1-hot)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # We initialize hx and cx\n",
    "        if torch.cuda.is_available():\n",
    "            hx = Variable(torch.zeros(1, 1, self.hidden_dim).cuda())\n",
    "            cx = Variable(torch.zeros(1, 1, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            hx = Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "            cx = Variable(torch.zeros(1, 1, self.hidden_dim))            \n",
    "        return (hx,cx)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # embeds = self.word_embeddings(sentence)  # input is already 1-hot\n",
    "        out, self.hidden = self.lstm(sentence.view(len(sentence), 1, -1), self.hidden)\n",
    "        out = self.hidden2out(out.view(len(sentence), -1))\n",
    "        scores = F.log_softmax(out)  # output log_softmax\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1730  1.7164 -1.9196  ...   0.4511  0.4606  0.4275\n",
      " 0.4723 -1.9316 -0.3462  ...   0.6750  0.9762  0.3099\n",
      "-0.4660  1.4636 -0.8490  ...  -1.1923 -1.1742 -0.8145\n",
      "          ...             â‹±             ...          \n",
      " 0.9543  1.7496 -0.4594  ...  -0.2349 -0.0735  0.6030\n",
      " 0.1273  0.6579 -0.0142  ...  -1.5914 -0.0784  1.7125\n",
      "-1.0749 -0.5387 -0.8794  ...   0.6246 -0.0168 -0.7994\n",
      "[torch.cuda.FloatTensor of size 32x93 (GPU 0)]\n",
      "\n",
      "torch.Size([32, 93])\n"
     ]
    }
   ],
   "source": [
    "RNN = LSTM_ABC(93, 100, 93).type(gpu_dtype)\n",
    "\n",
    "x = torch.randn(32, 93)\n",
    "x_var = Variable(x.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "\n",
    "print (x_var)\n",
    "log_prob = RNN(x_var)        # Feed it through the model! \n",
    "\n",
    "print (log_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch #:0   loss: 0.14189542829990387 \n",
      "batch #:10   loss: 0.14144934713840485 \n",
      "batch #:20   loss: 0.13604839146137238 \n",
      "batch #:30   loss: 0.13852986693382263 \n",
      "batch #:40   loss: 0.10861711949110031 \n",
      "batch #:50   loss: 0.1245763823390007 \n",
      "batch #:60   loss: 0.12714898586273193 \n",
      "batch #:70   loss: 0.0837089940905571 \n",
      "batch #:80   loss: 0.08174295723438263 \n",
      "batch #:90   loss: 0.08606889843940735 \n",
      "batch #:100   loss: 0.11090567708015442 \n",
      "batch #:110   loss: 0.09647621214389801 \n",
      "batch #:120   loss: 0.13509522378444672 \n",
      "batch #:130   loss: 0.11214707046747208 \n",
      "batch #:140   loss: 0.11632825434207916 \n",
      "batch #:150   loss: 0.11059252917766571 \n",
      "batch #:160   loss: 0.09504669904708862 \n",
      "batch #:170   loss: 0.0892217829823494 \n",
      "batch #:180   loss: 0.09378959238529205 \n",
      "batch #:190   loss: 0.07677535712718964 \n",
      "batch #:200   loss: 0.10473010689020157 \n",
      "batch #:210   loss: 0.09479682147502899 \n",
      "batch #:220   loss: 0.0707516223192215 \n",
      "batch #:230   loss: 0.0728142261505127 \n",
      "batch #:240   loss: 0.0848822146654129 \n",
      "batch #:250   loss: 0.08110861480236053 \n",
      "batch #:260   loss: 0.07178447395563126 \n",
      "batch #:270   loss: 0.16821442544460297 \n",
      "batch #:280   loss: 0.10246632993221283 \n",
      "batch #:290   loss: 0.08657868951559067 \n",
      "batch #:300   loss: 0.11562658846378326 \n",
      "batch #:310   loss: 0.09955636411905289 \n",
      "batch #:320   loss: 0.10873055458068848 \n",
      "batch #:330   loss: 0.0804995521903038 \n",
      "batch #:340   loss: 0.15793225169181824 \n",
      "batch #:350   loss: 0.08844765275716782 \n",
      "batch #:360   loss: 0.09174158424139023 \n",
      "batch #:370   loss: 0.11685901880264282 \n",
      "batch #:380   loss: 0.0894518569111824 \n",
      "batch #:390   loss: 0.10313573479652405 \n",
      "batch #:400   loss: 0.10754044353961945 \n",
      "batch #:410   loss: 0.09202875941991806 \n",
      "batch #:420   loss: 0.12265663594007492 \n",
      "batch #:430   loss: 0.13087666034698486 \n",
      "batch #:440   loss: 0.1275007128715515 \n",
      "batch #:450   loss: 0.10008161514997482 \n",
      "batch #:460   loss: 0.1074230968952179 \n",
      "batch #:470   loss: 0.09210900217294693 \n",
      "batch #:480   loss: 0.09215118736028671 \n",
      "batch #:490   loss: 0.11353098601102829 \n",
      "batch #:500   loss: 0.12960271537303925 \n",
      "batch #:510   loss: 0.11743317544460297 \n",
      "batch #:520   loss: 0.10822322964668274 \n",
      "batch #:530   loss: 0.10854656249284744 \n",
      "batch #:540   loss: 0.091668039560318 \n",
      "batch #:550   loss: 0.1001129299402237 \n",
      "batch #:560   loss: 0.09058484435081482 \n",
      "batch #:570   loss: 0.10039239376783371 \n",
      "batch #:580   loss: 0.12881134450435638 \n",
      "batch #:590   loss: 0.1237378790974617 \n",
      "batch #:600   loss: 0.10546494275331497 \n",
      "batch #:610   loss: 0.10304566472768784 \n",
      "batch #:620   loss: 0.0776176005601883 \n",
      "batch #:630   loss: 0.0790826752781868 \n",
      "batch #:640   loss: 0.09706462919712067 \n",
      "batch #:650   loss: 0.13319815695285797 \n",
      "batch #:660   loss: 0.08332351595163345 \n",
      "batch #:670   loss: 0.11363089829683304 \n",
      "batch #:680   loss: 0.09060302376747131 \n",
      "batch #:690   loss: 0.11720243841409683 \n",
      "batch #:700   loss: 0.09256356954574585 \n",
      "batch #:710   loss: 0.14280620217323303 \n",
      "batch #:720   loss: 0.10996238887310028 \n",
      "batch #:730   loss: 0.11445924639701843 \n",
      "batch #:740   loss: 0.12236861139535904 \n",
      "batch #:750   loss: 0.09956013411283493 \n",
      "batch #:760   loss: 0.09564347565174103 \n",
      "batch #:770   loss: 0.06919506192207336 \n",
      "batch #:780   loss: 0.06953040510416031 \n",
      "batch #:790   loss: 0.13281430304050446 \n",
      "batch #:800   loss: 0.1146661713719368 \n",
      "batch #:810   loss: 0.08096820116043091 \n",
      "batch #:820   loss: 0.0836285799741745 \n",
      "batch #:830   loss: 0.14168895781040192 \n",
      "batch #:840   loss: 0.09243837743997574 \n",
      "batch #:850   loss: 0.10284354537725449 \n",
      "batch #:860   loss: 0.05142892152070999 \n",
      "batch #:870   loss: 0.05213319882750511 \n",
      "batch #:880   loss: 0.10698851943016052 \n",
      "batch #:890   loss: 0.10408584028482437 \n",
      "batch #:900   loss: 0.11437462270259857 \n",
      "batch #:910   loss: 0.09875279664993286 \n",
      "batch #:920   loss: 0.09268860518932343 \n",
      "batch #:930   loss: 0.06948242336511612 \n",
      "batch #:940   loss: 0.10818592458963394 \n",
      "batch #:950   loss: 0.06445205956697464 \n",
      "batch #:960   loss: 0.07896784693002701 \n",
      "batch #:970   loss: 0.09584403783082962 \n",
      "batch #:980   loss: 0.10683628171682358 \n",
      "batch #:990   loss: 0.11301055550575256 \n",
      "batch #:1000   loss: 0.10773588716983795 \n",
      "batch #:1010   loss: 0.125864639878273 \n",
      "batch #:1020   loss: 0.10779368132352829 \n",
      "batch #:1030   loss: 0.0950603112578392 \n",
      "batch #:1040   loss: 0.11947707086801529 \n",
      "batch #:1050   loss: 0.10426871478557587 \n",
      "batch #:1060   loss: 0.07797861099243164 \n",
      "batch #:1070   loss: 0.07466986030340195 \n",
      "batch #:1080   loss: 0.06220481917262077 \n",
      "batch #:1090   loss: 0.1013139858841896 \n",
      "batch #:1100   loss: 0.08027887344360352 \n",
      "batch #:1110   loss: 0.09578245133161545 \n",
      "batch #:1120   loss: 0.06575077027082443 \n",
      "batch #:1130   loss: 0.05287327989935875 \n",
      "batch #:1140   loss: 0.08899267017841339 \n",
      "batch #:1150   loss: 0.11247985064983368 \n",
      "batch #:1160   loss: 0.11660788208246231 \n",
      "batch #:1170   loss: 0.0870421752333641 \n",
      "batch #:1180   loss: 0.07956349104642868 \n",
      "batch #:1190   loss: 0.0842231884598732 \n",
      "batch #:1200   loss: 0.10621420294046402 \n",
      "batch #:1210   loss: 0.08086243271827698 \n",
      "batch #:1220   loss: 0.11078755557537079 \n",
      "batch #:1230   loss: 0.11323902010917664 \n",
      "batch #:1240   loss: 0.09929244965314865 \n",
      "batch #:1250   loss: 0.09015976637601852 \n",
      "batch #:1260   loss: 0.09726478904485703 \n",
      "batch #:1270   loss: 0.08342941850423813 \n",
      "batch #:1280   loss: 0.0922379195690155 \n",
      "batch #:1290   loss: 0.08550795167684555 \n",
      "batch #:1300   loss: 0.09281376004219055 \n",
      "batch #:1310   loss: 0.06900440901517868 \n",
      "batch #:1320   loss: 0.09408474713563919 \n",
      "batch #:1330   loss: 0.08244769275188446 \n",
      "batch #:1340   loss: 0.06768672913312912 \n",
      "batch #:1350   loss: 0.06732644885778427 \n",
      "batch #:1360   loss: 0.0830211415886879 \n",
      "batch #:1370   loss: 0.09429904818534851 \n",
      "batch #:1380   loss: 0.09385942667722702 \n",
      "batch #:1390   loss: 0.08997596055269241 \n",
      "batch #:1400   loss: 0.08141624182462692 \n",
      "batch #:1410   loss: 0.08965419977903366 \n",
      "batch #:1420   loss: 0.06526925414800644 \n",
      "batch #:1430   loss: 0.08252903074026108 \n",
      "batch #:1440   loss: 0.0911833792924881 \n",
      "batch #:1450   loss: 0.08345221728086472 \n",
      "batch #:1460   loss: 0.08928930014371872 \n",
      "batch #:1470   loss: 0.0734461322426796 \n",
      "batch #:1480   loss: 0.07033713907003403 \n",
      "batch #:1490   loss: 0.05401048809289932 \n",
      "batch #:1500   loss: 0.0665808692574501 \n",
      "batch #:1510   loss: 0.07426172494888306 \n",
      "batch #:1520   loss: 0.06814452260732651 \n",
      "batch #:1530   loss: 0.07115078717470169 \n",
      "batch #:1540   loss: 0.0658046305179596 \n",
      "batch #:1550   loss: 0.09597965329885483 \n",
      "batch #:1560   loss: 0.06845254451036453 \n",
      "batch #:1570   loss: 0.06962817162275314 \n",
      "batch #:1580   loss: 0.07402051985263824 \n",
      "batch #:1590   loss: 0.08849166333675385 \n",
      "batch #:1600   loss: 0.06960216164588928 \n",
      "batch #:1610   loss: 0.10685653239488602 \n",
      "batch #:1620   loss: 0.08907469362020493 \n",
      "batch #:1630   loss: 0.09930828958749771 \n",
      "batch #:1640   loss: 0.07838663458824158 \n",
      "batch #:1650   loss: 0.08899789303541183 \n",
      "batch #:1660   loss: 0.06750164180994034 \n",
      "batch #:1670   loss: 0.05481906607747078 \n",
      "batch #:1680   loss: 0.0721418634057045 \n",
      "batch #:1690   loss: 0.06489182263612747 \n",
      "batch #:1700   loss: 0.06325683742761612 \n",
      "batch #:1710   loss: 0.09150681644678116 \n",
      "batch #:1720   loss: 0.07804115116596222 \n",
      "batch #:1730   loss: 0.08445888012647629 \n",
      "batch #:1740   loss: 0.06154586374759674 \n",
      "batch #:1750   loss: 0.09605433791875839 \n",
      "batch #:1760   loss: 0.11229466646909714 \n",
      "batch #:1770   loss: 0.0988563522696495 \n",
      "batch #:1780   loss: 0.08794638514518738 \n",
      "batch #:1790   loss: 0.07520326226949692 \n",
      "batch #:1800   loss: 0.08397088944911957 \n",
      "batch #:1810   loss: 0.08638526499271393 \n",
      "batch #:1820   loss: 0.07237917184829712 \n",
      "batch #:1830   loss: 0.06497213989496231 \n",
      "batch #:1840   loss: 0.09885499626398087 \n",
      "batch #:1850   loss: 0.07150953263044357 \n",
      "batch #:1860   loss: 0.05869494006037712 \n",
      "batch #:1870   loss: 0.09930277615785599 \n",
      "batch #:1880   loss: 0.07644020766019821 \n",
      "batch #:1890   loss: 0.06134757027029991 \n",
      "batch #:1900   loss: 0.0609869584441185 \n",
      "batch #:1910   loss: 0.06805792450904846 \n",
      "batch #:1920   loss: 0.05450165644288063 \n",
      "batch #:1930   loss: 0.0535995177924633 \n",
      "batch #:1940   loss: 0.03771868720650673 \n",
      "batch #:1950   loss: 0.06292112916707993 \n",
      "batch #:1960   loss: 0.07513085007667542 \n",
      "batch #:1970   loss: 0.10300453752279282 \n",
      "batch #:1980   loss: 0.08433952927589417 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch #:1990   loss: 0.0883973240852356 \n",
      "batch #:2000   loss: 0.0725988820195198 \n",
      "batch #:2010   loss: 0.08308902382850647 \n",
      "batch #:2020   loss: 0.0676867887377739 \n",
      "batch #:2030   loss: 0.06909359991550446 \n",
      "batch #:2040   loss: 0.08613201230764389 \n",
      "batch #:2050   loss: 0.08782829344272614 \n",
      "batch #:2060   loss: 0.06889494508504868 \n",
      "batch #:2070   loss: 0.0725112184882164 \n",
      "batch #:2080   loss: 0.04392377659678459 \n",
      "batch #:2090   loss: 0.07420545071363449 \n",
      "batch #:2100   loss: 0.04618201404809952 \n",
      "batch #:2110   loss: 0.08181831240653992 \n",
      "batch #:2120   loss: 0.06723026186227798 \n",
      "batch #:2130   loss: 0.06005096435546875 \n",
      "batch #:2140   loss: 0.07263901829719543 \n",
      "batch #:2150   loss: 0.06917593628168106 \n",
      "batch #:2160   loss: 0.06018983945250511 \n",
      "batch #:2170   loss: 0.050410106778144836 \n",
      "batch #:2180   loss: 0.03736766800284386 \n",
      "batch #:2190   loss: 0.08210954070091248 \n",
      "batch #:2200   loss: 0.060390621423721313 \n",
      "batch #:2210   loss: 0.0630074068903923 \n",
      "batch #:2220   loss: 0.06097196415066719 \n",
      "batch #:2230   loss: 0.04712377116084099 \n",
      "batch #:2240   loss: 0.05746262148022652 \n",
      "batch #:2250   loss: 0.06178486347198486 \n",
      "batch #:2260   loss: 0.0600595586001873 \n",
      "batch #:2270   loss: 0.0825384333729744 \n",
      "batch #:2280   loss: 0.0517495758831501 \n",
      "batch #:2290   loss: 0.061600107699632645 \n",
      "batch #:2300   loss: 0.07162792235612869 \n",
      "batch #:2310   loss: 0.06296029686927795 \n",
      "batch #:2320   loss: 0.09060857445001602 \n",
      "batch #:2330   loss: 0.06080523878335953 \n",
      "batch #:2340   loss: 0.06676559150218964 \n",
      "batch #:2350   loss: 0.07142090052366257 \n",
      "batch #:2360   loss: 0.05997184291481972 \n",
      "batch #:2370   loss: 0.10359374433755875 \n",
      "batch #:2380   loss: 0.08499960601329803 \n",
      "batch #:2390   loss: 0.07996832579374313 \n",
      "batch #:2400   loss: 0.07395496219396591 \n",
      "batch #:2410   loss: 0.03517713025212288 \n",
      "batch #:2420   loss: 0.06457661837339401 \n",
      "batch #:2430   loss: 0.04677779972553253 \n",
      "batch #:2440   loss: 0.0496550090610981 \n",
      "batch #:2450   loss: 0.05154276639223099 \n",
      "batch #:2460   loss: 0.04980871081352234 \n",
      "batch #:2470   loss: 0.07492846995592117 \n",
      "batch #:2480   loss: 0.06274424493312836 \n",
      "batch #:2490   loss: 0.08659786731004715 \n",
      "batch #:2500   loss: 0.07719037681818008 \n",
      "batch #:2510   loss: 0.040662772953510284 \n",
      "batch #:2520   loss: 0.05203466862440109 \n",
      "batch #:2530   loss: 0.05996895581483841 \n",
      "batch #:2540   loss: 0.0399317629635334 \n",
      "batch #:2550   loss: 0.06030518189072609 \n",
      "batch #:2560   loss: 0.06493180990219116 \n",
      "batch #:2570   loss: 0.03405751287937164 \n",
      "batch #:2580   loss: 0.05428815633058548 \n",
      "batch #:2590   loss: 0.06835083663463593 \n",
      "batch #:2600   loss: 0.043166983872652054 \n",
      "batch #:2610   loss: 0.07803858071565628 \n",
      "batch #:2620   loss: 0.08479434251785278 \n",
      "batch #:2630   loss: 0.06801199913024902 \n",
      "batch #:2640   loss: 0.07076606154441833 \n",
      "batch #:2650   loss: 0.052869830280542374 \n",
      "batch #:2660   loss: 0.04500548169016838 \n",
      "batch #:2670   loss: 0.0679425448179245 \n",
      "batch #:2680   loss: 0.07218791544437408 \n",
      "batch #:2690   loss: 0.04334817826747894 \n",
      "batch #:2700   loss: 0.07387847453355789 \n",
      "batch #:2710   loss: 0.053065769374370575 \n",
      "batch #:2720   loss: 0.04836651310324669 \n",
      "batch #:2730   loss: 0.047495920211076736 \n",
      "batch #:2740   loss: 0.05052556097507477 \n",
      "batch #:2750   loss: 0.04521739110350609 \n",
      "batch #:2760   loss: 0.02899327129125595 \n",
      "batch #:2770   loss: 0.00961416494101286 \n",
      "batch #:2780   loss: 0.0717306062579155 \n",
      "batch #:2790   loss: 0.06719483435153961 \n",
      "batch #:2800   loss: 0.051925912499427795 \n",
      "batch #:2810   loss: 0.09610863029956818 \n",
      "batch #:2820   loss: 0.04479166492819786 \n",
      "batch #:2830   loss: 0.07636699825525284 \n",
      "batch #:2840   loss: 0.03751889616250992 \n",
      "batch #:2850   loss: 0.08584047853946686 \n",
      "batch #:2860   loss: 0.08170747756958008 \n",
      "batch #:2870   loss: 0.10230083763599396 \n",
      "batch #:2880   loss: 0.06399338692426682 \n",
      "batch #:2890   loss: 0.0625070184469223 \n",
      "batch #:2900   loss: 0.06922565400600433 \n",
      "batch #:2910   loss: 0.05800357833504677 \n",
      "batch #:2920   loss: 0.03726036474108696 \n",
      "batch #:2930   loss: 0.06276681274175644 \n",
      "batch #:2940   loss: 0.09085976332426071 \n",
      "batch #:2950   loss: 0.06913471966981888 \n",
      "batch #:2960   loss: 0.04656652733683586 \n",
      "batch #:2970   loss: 0.09601882100105286 \n",
      "batch #:2980   loss: 0.02450200729072094 \n",
      "batch #:2990   loss: 0.07700703293085098 \n",
      "batch #:3000   loss: 0.048093460500240326 \n",
      "batch #:3010   loss: 0.07126401364803314 \n",
      "batch #:3020   loss: 0.1298675239086151 \n",
      "batch #:3030   loss: 0.09056717157363892 \n",
      "batch #:3040   loss: 0.10173505544662476 \n",
      "batch #:3050   loss: 0.03370622918009758 \n",
      "batch #:3060   loss: 0.05832796171307564 \n",
      "batch #:3070   loss: 0.06273596733808517 \n",
      "batch #:3080   loss: 0.022812196984887123 \n",
      "batch #:3090   loss: 0.05699465051293373 \n",
      "batch #:3100   loss: 0.032208152115345 \n",
      "batch #:3110   loss: 0.07792967557907104 \n",
      "batch #:3120   loss: 0.07255028188228607 \n",
      "batch #:3130   loss: 0.05699203908443451 \n",
      "batch #:3140   loss: 0.07650383561849594 \n",
      "batch #:3150   loss: 0.02622501738369465 \n",
      "batch #:3160   loss: 0.05711914598941803 \n",
      "batch #:3170   loss: 0.05983402207493782 \n",
      "batch #:3180   loss: 0.0637502521276474 \n",
      "batch #:3190   loss: 0.03769867494702339 \n",
      "batch #:3200   loss: 0.07408572733402252 \n",
      "batch #:3210   loss: 0.051850177347660065 \n",
      "batch #:3220   loss: 0.07249130308628082 \n",
      "batch #:3230   loss: 0.051524896174669266 \n",
      "batch #:3240   loss: 0.05447132885456085 \n",
      "batch #:3250   loss: 0.08157707750797272 \n",
      "batch #:3260   loss: 0.04662581533193588 \n",
      "batch #:3270   loss: 0.09050322324037552 \n",
      "batch #:3280   loss: 0.10949727147817612 \n",
      "batch #:3290   loss: 0.0764995664358139 \n",
      "batch #:3300   loss: 0.06856483966112137 \n",
      "batch #:3310   loss: 0.098223976790905 \n",
      "batch #:3320   loss: 0.0669122263789177 \n",
      "batch #:3330   loss: 0.058918021619319916 \n",
      "batch #:3340   loss: 0.03195681795477867 \n",
      "batch #:3350   loss: 0.07252174615859985 \n",
      "batch #:3360   loss: 0.022809673100709915 \n",
      "batch #:3370   loss: 0.06734143942594528 \n",
      "batch #:3380   loss: 0.05158593878149986 \n",
      "batch #:3390   loss: 0.08909126371145248 \n",
      "batch #:3400   loss: 0.07973992079496384 \n",
      "batch #:3410   loss: 0.08254306018352509 \n",
      "batch #:3420   loss: 0.059956204146146774 \n",
      "batch #:3430   loss: 0.0719510167837143 \n",
      "batch #:3440   loss: 0.08436653763055801 \n",
      "batch #:3450   loss: 0.01999981887638569 \n",
      "batch #:3460   loss: 0.05626419559121132 \n",
      "batch #:3470   loss: 0.060175105929374695 \n",
      "batch #:3480   loss: 0.07508955895900726 \n",
      "batch #:3490   loss: 0.09453791379928589 \n",
      "batch #:3500   loss: 0.03966236487030983 \n",
      "batch #:3510   loss: 0.02388807199895382 \n",
      "batch #:3520   loss: 0.10530601441860199 \n",
      "batch #:3530   loss: 0.08870676159858704 \n",
      "batch #:3540   loss: 0.06277211755514145 \n",
      "batch #:3550   loss: 0.08687479048967361 \n",
      "batch #:3560   loss: 0.09631819278001785 \n",
      "batch #:3570   loss: 0.13032932579517365 \n",
      "batch #:3580   loss: 0.06797564774751663 \n",
      "batch #:3590   loss: 0.08983813971281052 \n",
      "batch #:3600   loss: 0.10178455710411072 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3be41640ac70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# NLLoss accept index as target, not one-hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 1\n",
    "plot_every = 10\n",
    "input_dim = 93\n",
    "output_dim = 93\n",
    "hidden_dim = 100\n",
    "# n_layers = 1\n",
    "lr = 0.001\n",
    "verbose = False\n",
    "\n",
    "# Create an instance of the LSTM and set optimizer and loss function\n",
    "model = LSTM_ABC(input_dim, hidden_dim, output_dim).type(gpu_dtype)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# The output of the model is softmax log prob, so the loss function is \n",
    "# NLL (negative log likelihood)\n",
    "loss_function = nn.NLLLoss()   \n",
    "\n",
    "all_losses = []  # keep track of loss history\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # loss = train(*random_training_set())   \n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    for t, sentence in enumerate(loader_train):\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        hidden = model.init_hidden()\n",
    "       \n",
    "        # Step 2. Get our inputs and targets ready for the network\n",
    "        # input will be fed character by character into LSTM\n",
    "        inp = sentence[:-1,:]\n",
    "        if verbose:\n",
    "            print(display_batch(inp, char_dict)) \n",
    "        # LSTM output will be compared against the target, which is \n",
    "        target = sentence[1:,:].type(torch.LongTensor)  \n",
    "        if verbose:\n",
    "            print(display_batch(target, char_dict))\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        target = target.cuda()\n",
    "        scores = model(Variable(inp.type(gpu_dtype)))\n",
    "        if verbose:\n",
    "            print (scores.size())\n",
    "            print (target.size())\n",
    "            \n",
    "       # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        target_index = torch.max(target,1)[1]  # NLLoss accept index as target, not one-hot\n",
    "        loss = loss_function(scores, Variable(target_index))\n",
    "        loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 50)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if t%10 == 0:\n",
    "            print (\"batch #:{}   loss: {} \".format(t, loss.data[0]/len(sentence)))\n",
    "    \n",
    "        loss_avg += loss.data[0] / len(sentence)\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[(%d %d%%) %.4f]' % (epoch, epoch / n_epochs * 100, loss))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='X', predict_len=100, temperature=0.8):\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    prime_input = letterToTensor(prime_str, char_dict)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _ = model(prime_input[p])\n",
    "     \n",
    "    inp = prime_input   # The prime character is input to LSTM\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output = model(Variable(inp.type(gpu_dtype)))\n",
    "                \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = char_dict[top_i]\n",
    "        predicted += predicted_char\n",
    "        \n",
    "        inp = letterToTensor(predicted_char, char_dict)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:116\n",
      "L:1/8\n",
      "O:1\n",
      "R:3e\"stoderit on coutielsove?.er tonceltames pari? sarnscitem(itomte paisle 12e cELLO\n"
     ]
    }
   ],
   "source": [
    "predicted = evaluate()\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
