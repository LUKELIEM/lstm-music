{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA4 - LSTM for Music Generation (Version 1)\n",
    "\n",
    "We will train an LSTM to generate music using the ABC Notation.\n",
    "\n",
    "This approach is not implemented the right way. The music generated is garbage! \n",
    "\n",
    "X: >h fige  \n",
    "W:.antie, :- Z:Fn?an0tan !atove? :Traneenretc  \n",
    "O:T:b9 :Larg? ma:qra>jo:  \n",
    "cT0and/g  \n",
    "O:Fa | d2A  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.5.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "import platform\n",
    "import random\n",
    "import pickle as pickle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def num_datapoints(filename):\n",
    "    \"\"\"\n",
    "    Given the name of a CVS file, the function returns the number of rows of data.\n",
    "    Inputs:\n",
    "    - filename: name of CVS file\n",
    "    Returns:\n",
    "    The number of rows of data\n",
    "    \"\"\"\n",
    "    \n",
    "    row_count = 0\n",
    "    \n",
    "    f = open(filename, 'rt')\n",
    "    for line in f:\n",
    "        row_count += 1\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    return row_count\n",
    "\n",
    "def num_chars(filename):\n",
    "    \"\"\"\n",
    "    Given the name of a CVS file, the function returns the number of characters in the file.\n",
    "    Inputs:\n",
    "    - filename: name of CVS file\n",
    "    Returns:\n",
    "    The number of rows of data\n",
    "    \"\"\"\n",
    "    \n",
    "    char_count = 0\n",
    "    \n",
    "    f = open(filename, 'rt')\n",
    "    for line in f:\n",
    "        for c in line:\n",
    "            char_count += 1\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    return char_count\n",
    "\n",
    "\n",
    "def extract_characters(filename):\n",
    "    \"\"\"\n",
    "    Given the name of a file, the function returns a list of unique characters in the file.\n",
    "    Inputs:\n",
    "    - filename: name of CVS file\n",
    "    Returns:\n",
    "    A list of unique character\n",
    "    \"\"\"\n",
    "    characters = []\n",
    "    f = open(filename, 'rt')\n",
    "\n",
    "    for line in f:\n",
    "        for c in line:\n",
    "            if c not in characters:\n",
    "                characters.append(c)\n",
    "    f.close()        \n",
    "    return characters\n",
    "\n",
    "\n",
    "def letterToIndex(letter, char_dict):\n",
    "    \"\"\"\n",
    "    Find letter index from all_letters, e.g. \"\\t\" = 0\n",
    "    Inputs:\n",
    "    - letter\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    Index of the letter in the dictionary\n",
    "    \"\"\"\n",
    "    return char_dict.index(letter)\n",
    "\n",
    "def letterToTensor(letter, char_dict):\n",
    "    \"\"\"\n",
    "    Turn a letter into a <1 x len(char_dict)> Tensor representing its one-hot encoding.\n",
    "    Inputs:\n",
    "    - letter\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    tensor: One-hot encoding of the letter\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(1, len(char_dict))\n",
    "    tensor[0, letterToIndex(letter, char_dict)] = 1\n",
    "    return tensor.view(1,-1)\n",
    "\n",
    "\n",
    "def tensorToLetter(tensor, char_dict):\n",
    "    \"\"\"\n",
    "    Turn a one-hot <1 x len(char_dict)> Tensor back to the letter\n",
    "    Inputs:\n",
    "    - letter\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    letter: One-hot encoding of the letter\n",
    "    \"\"\"\n",
    "    index = torch.max(tensor.view(1,-1),1)[1]   # force tensor to 1xN dimension\n",
    "    return char_dict[index.numpy()[0]]\n",
    "\n",
    "\n",
    "def lineToTensor(line, char_dict):\n",
    "    \"\"\"\n",
    "    Turn a line into an array of one-hot tensors <line_length x 1 x len(char_dict)>\n",
    "    Inputs:\n",
    "    - line\n",
    "    - char_dict: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    tensor: an array of one-hot tensors\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(line), len(char_dict))\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li, letterToIndex(letter, char_dict)] = 1\n",
    "    return tensor\n",
    "\n",
    "def display_batch(minibatch, char_dict):\n",
    "    line = ''\n",
    "    for tensor in minibatch:\n",
    "        char = tensorToLetter(tensor, char_dict)\n",
    "        line = line + char\n",
    "    return line\n",
    "\n",
    "def generateCorpus(filename, dictionary):\n",
    "    \"\"\"\n",
    "    Take all the content in a text file and generate a corpus of one-hot encodings.\n",
    "    Inputs:\n",
    "    - filename\n",
    "    - dictionary: A dictionary of all possible characters\n",
    "    Returns:\n",
    "    corpus: a tensor of <total_chars, 1, len(dictionary)>\n",
    "    \"\"\"\n",
    "    \n",
    "    total_chars = num_chars(filename)  # figure out how many char in the file\n",
    "    corpus = torch.zeros(total_chars, len(dictionary))\n",
    "    i = 0\n",
    "\n",
    "    f = open(filename, 'rt')\n",
    "    for line in f:\n",
    "        for c in line:\n",
    "            corpus[i, letterToIndex(c, dictionary)] = 1  # one-hot encoding\n",
    "            i += 1\n",
    "            \n",
    "    f.close()  \n",
    "    return corpus\n",
    "\n",
    "def prepare_sequence(batch):\n",
    "    seq = []\n",
    "    for i in range(batch.size([0])):\n",
    "        seq.append(batch[i,:])\n",
    "        tensor = torch.LongTensor(seq)\n",
    "    return Variable(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dictionary\n",
    "\n",
    "We parse through input.txt and extract all unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "\n",
      "X:1\n",
      "\n",
      "T: La Montfarine\n",
      "\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-07-24\n",
      "\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\n",
      "\n",
      "M: 4/4\n",
      "\n",
      "L: 1/8\n",
      "\n",
      "Q:1/4=186\n",
      "\n",
      "FGF B=AG G=AG F2F FGF {F}F2E EFE|\n",
      "\n",
      "{E}E2D FGF B=AG G=AG {F}F2F FED C2G D2E|F3 {F}F/2 ED E3/2D/2|\n",
      "\n",
      "<class 'str'>\n",
      "18711\n",
      "501470\n",
      "['\\t', '\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "filename = 'input.txt'\n",
    "\n",
    "inputfile = open(filename, 'rt')\n",
    "\n",
    "# explore the content of the input.txt file\n",
    "i = 0\n",
    "for line in inputfile:\n",
    "    if i < 10:\n",
    "        print (line)\n",
    "        i += 1\n",
    "    \n",
    "inputfile.close()\n",
    "\n",
    "print (type(line))  # line are read in as strings\n",
    "\n",
    "print (num_datapoints(filename))  # 18711 rows in the file\n",
    "print (num_chars(filename))  # 501470 characters in the file\n",
    "\n",
    "char_dict = sorted(extract_characters(filename))   # 93 unique characters\n",
    "\n",
    "print (char_dict)\n",
    "print (len(char_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([501470, 93])\n",
      "<start>\n",
      "X:1\n",
      "T: La Montfarine\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-07-24\n",
      "Z:Pour toute o\n"
     ]
    }
   ],
   "source": [
    "filename = 'input.txt'\n",
    "corpus = generateCorpus(filename, char_dict)\n",
    "\n",
    "print (corpus.size())\n",
    "print(display_batch(corpus[0:100,:], char_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Split\n",
    "\n",
    "We will split characters from the input.txt 80-20 into test and val datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 401470\n",
    "NUM_VAL = 100000\n",
    "\n",
    "loader_train = DataLoader(corpus, batch_size=32, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "loader_val = DataLoader(corpus, batch_size=32, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM\n",
    "\n",
    "Pytorch’s LSTM expects all of its inputs to be 3D tensors:  \n",
    "\n",
    "* The first axis is the sequence itself  \n",
    "* the second indexes instances in the mini-batch, and  \n",
    "* the third indexes elements of the input.  \n",
    "\n",
    "Below, we build an LSTM, and run a sequence of 5 inputs through it generating 5 outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_dtype = torch.FloatTensor # the CPU datatype\n",
    "gpu_dtype = torch.cuda.FloatTensor  # the GPU datatype\n",
    "\n",
    "class LSTM_ABC(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTM_ABC, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # We encode the character as 1-hot, so skip this step\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes 1-hot as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to output (1-hot)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # We initialize hx and cx\n",
    "        if torch.cuda.is_available():\n",
    "            hx = Variable(torch.zeros(1, 1, self.hidden_dim).cuda())\n",
    "            cx = Variable(torch.zeros(1, 1, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            hx = Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "            cx = Variable(torch.zeros(1, 1, self.hidden_dim))            \n",
    "        return (hx,cx)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # embeds = self.word_embeddings(sentence)  # input is already 1-hot\n",
    "        out, self.hidden = self.lstm(sentence.view(len(sentence), 1, -1), self.hidden)\n",
    "        out = self.hidden2out(out.view(len(sentence), -1))\n",
    "        scores = F.log_softmax(out)  # output log_softmax\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 6.6070e-01  1.3356e+00  1.1638e+00  ...   5.8624e-01  8.0986e-01  8.7389e-01\n",
      "-3.3408e-01 -1.2273e-01  1.2107e-01  ...   4.9709e-01 -1.1391e+00 -6.5605e-01\n",
      "-1.6161e+00  2.6525e+00 -8.7807e-01  ...  -1.4024e+00 -5.9409e-01 -6.7979e-01\n",
      "                ...                   ⋱                   ...                \n",
      " 1.4023e+00  5.2695e-01  4.0012e-01  ...   9.5999e-01 -7.5821e-01  7.5429e-01\n",
      "-1.9691e+00  3.4790e-01  8.4739e-01  ...  -1.9768e+00 -3.9197e-01 -5.3130e-01\n",
      "-1.0685e+00 -1.0034e+00  3.8652e-01  ...   1.3969e+00  1.2622e-01  2.9221e-01\n",
      "[torch.cuda.FloatTensor of size 32x93 (GPU 0)]\n",
      "\n",
      "torch.Size([32, 93])\n"
     ]
    }
   ],
   "source": [
    "RNN = LSTM_ABC(93, 100, 93).type(gpu_dtype)\n",
    "\n",
    "x = torch.randn(32, 93)\n",
    "x_var = Variable(x.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "\n",
    "print (x_var)\n",
    "log_prob = RNN(x_var)        # Feed it through the model! \n",
    "\n",
    "print (log_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='X', predict_len=100, temperature=0.8):\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    prime_input = letterToTensor(prime_str, char_dict)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _ = model(prime_input[p])\n",
    "     \n",
    "    inp = prime_input   # The prime character is input to LSTM\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output = model(Variable(inp.type(gpu_dtype)))\n",
    "                \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = char_dict[top_i]\n",
    "        predicted += predicted_char\n",
    "        \n",
    "        inp = letterToTensor(predicted_char, char_dict)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch #:0   loss: 0.14146967232227325 \n",
      "Primed with X:\n",
      "X~YyTqo1 xS:5+]1^ysgX4cxsZt1Fem8>.,|^Nn#t'3Q1D !'w\n",
      "<p [M\"_-*W D(mZRqDiF'cfHC(Hr8#JT6R'{h3)?mEF~WQ.J_X\n",
      "Primed with <:\n",
      "<IA2xl-M4,k4faK-C\\An'QWH+pWe@3neW!>!bn7e8/GOk_Jkc}nH|7dMt#(l)(lV:TN!aQ>,)&G438a<g /xj+.Qd5)5q\"u6vs\n",
      "Aw\n",
      "batch #:100   loss: 0.11139734834432602 \n",
      "Primed with X:\n",
      "X2 >Buddd tc24ee!4dB |  d|d| B|2 |2\n",
      "3 i  -c:a|c : e cBP B2e_e> frfe |c42\n",
      "2 2|d dB|   t|2d F  |e \n",
      " faB\n",
      "Primed with <:\n",
      "<2a\n",
      "2p24 2c1 ||BAr22 | F|22| dz2B2  2BeGFo _21|Bu2  r|2Gb2|F2gg , e4fA2 2cE:][\n",
      "-e2B|e 2 224B| cd ||f2\n",
      "batch #:200   loss: 0.10474832355976105 \n",
      "Primed with X:\n",
      "Xc/ .e/./B/!///d/c//B/c_c//c/c/6 B= nc! =cG //2)/ ./ g +/gg/eac|B// /B g//  B=2/ //d/! B!///Bef/>+Bd/\n",
      "Primed with <:\n",
      "<B Bd /cBcA/ d  B.//c cc!= /BBcB // d / cd/! /ce|d/ 2//e.c  B  d/f//df//B//dc/=Bcd.d//d/|dB=dB./|.e d\n",
      "batch #:300   loss: 0.11632761359214783 \n",
      "Primed with X:\n",
      "X  e! ue|e  2G| |  BBd: |/ |e bed  2!|dc|cr|/de|  : rd(o2  {  d )|V eg|c.dec3)ec+B T  !2B | ef\n",
      "t ! ||\n",
      "Primed with <:\n",
      "<( fefB|ue f  |// e= ucfc c g/ee  z/ |c/6|Bd3b|sGTfFf (hc |2 2 e fc2= 2cc| c)| Beg d2z fddB=c.e3c|/:|\n",
      "batch #:400   loss: 0.10693541169166565 \n",
      "Primed with X:\n",
      "X \n",
      " E:L >t dgeQBd|MG )en | 3F/(sfgc]ddgr|  l|i|| 2( -g l/ G2Br \n",
      "G|/G!Fa.|| g-33}/3dKc t   Bcd f(d3d \n",
      "\n",
      "Primed with <:\n",
      "< ) =Bc(G=G:a|/2 d iBci'|:/x> {| -egoA Adg22fBc!=d  / |Bd/:g \n",
      "|ibtgd/  NnBG2d c 2(>ff| (3 dgB2 f2t = \n",
      "batch #:500   loss: 0.13065053522586823 \n",
      "Primed with X:\n",
      "X|ce2r:22|sK(|Ac| i/:22B r /2/ c| |2Dc42|  AnBr}22aan:/e F |\n",
      "s2  B2d2|)geo2| er|BG2C|o 2cdc 3GcBBdi|e\n",
      "Primed with <:\n",
      "<r:iC/|d\n",
      "\n",
      "G/22/e2 |242a1efer/2a2d 222c(-scf 4|t \n",
      "2G\n",
      "/dd|2B  2:\n",
      " 1|2B aed/AocG2f e2|| BoA  |d\n",
      "|G   re2\n",
      "batch #:600   loss: 0.1086760014295578 \n",
      "Primed with X:\n",
      "X>=|2daed( >\n",
      ":c| oBle|\n",
      "@| |Ec>|caf+s|/2  2BD  2 |  : fe>t2c dFoiaBm\n",
      "Avl:fiR\n",
      "G\n",
      "o:/ |Bc>ot|de4@Br]ts GT\n",
      "Primed with <:\n",
      "<e B2ef Gd[r|eBzA4GfF2=n /va | \n",
      "fd\n",
      "fe\n",
      " Gi/2Bc|  :?e   (m  teA>ce:>G\n",
      "c |tc c 2  e2>:]di a ,/ 4|   :2=N\n",
      "batch #:700   loss: 0.09535010904073715 \n",
      "Primed with X:\n",
      "Xg:  02h \n",
      " l \n",
      "jas  r  \n",
      "dl cB|d : BndoB | | aB\n",
      "o-3cf |:B |  c\n",
      "4i ad e>ar 23f| | b1\n",
      " n  />| \n",
      "d | e A2Az\n",
      "Primed with <:\n",
      "<d2e2aG|  2 8 d| e)2td \n",
      " rzz|  fc Atrc 2 \n",
      "+d Bd G u2 L/a|e/ . l  |B z  |c2G f8u  cd 4Zd/o  0u f  X . \n",
      "batch #:800   loss: 0.11493588984012604 \n",
      "Primed with X:\n",
      "XrB ofB Bt dbe/ |: c2cBPo5iAa b| dTA 2 B B.c z{A |BF\n",
      "ie /c d d2 | C>z2eA4c| |2-f2dB2 eh  |f/oceFd2cGe\n",
      "Primed with <:\n",
      "<C f | d l3 | |\n",
      "!AeT|dof fa  |\n",
      "ep  [ ptgeeADd |doTu.t A \n",
      "ZcG2g/ 2z PApdrMd2| reK /2 g dOEo2f4 :\n",
      "Bc \n",
      "3\n",
      "batch #:900   loss: 0.12222706526517868 \n",
      "Primed with X:\n",
      "X)|| | BdABFdBcB|s| | | /2|G/  |A|B(|\n",
      "2|tG|)B3M|r|2B|^|B)| |4noara- |B)aebfB6|f(!si4G|rMdr| A|!|B|2:|\n",
      "Primed with <:\n",
      "< Ma| /cFc |n|2(2 | |2-|gdd dGM|B)A| |2|2eoBG|G2/|c|1|)2B|-e|G3B|BM|G2f| |Bcd|2F :MBfA| c2|2\n",
      "|\n",
      "B{2c| \n",
      "batch #:1000   loss: 0.10974360257387161 \n",
      "Primed with X:\n",
      "Xe\n",
      "i roA3pvteBiaofdeB\n",
      "rttcoe rlo e en|I lnltfr 0o ei cr\n",
      "\n",
      "a tLirasl2 - :i:u m>s se \\rorroNi\n",
      "lt\n",
      "isaf to\n",
      "Primed with <:\n",
      "<rr of era\n",
      "yns u ve| \n",
      "t:o\n",
      " /v\n",
      "/deAaa>B | dee(tevoseou\n",
      "ll21\n",
      "e sno 0tL?if/ rw tdz mir Af enL_dv nar2n t\n",
      "batch #:1100   loss: 0.07773428410291672 \n",
      "Primed with X:\n",
      "XBEus- | Br-4e C2d!2 d2 | d| e> | c| : / c cfBAc | /2 | 23 d | ) A2 | d> | B r B2 | /2 | | | d c  | B\n",
      "Primed with <:\n",
      "<2B f2 | euB | | | B | | |2B2 | B2Be A c | B2cc]2-)\n",
      "pA d | | d F | A | |FBBf | BGr|2 | | d2 A | >Gc2 \n",
      "batch #:1200   loss: 0.10634098947048187 \n",
      "Primed with X:\n",
      "XB2ef G6 | Bb3| mF :n>et Fseoreeneoealo_iilhae2ecaitrr llaxaAerliieMaeetictit:einstiouogertenubornGo]\n",
      "Primed with <:\n",
      "<es:otasaebfienEro\n",
      "blscapuibeitce hirirraieoielduieK\n",
      "sdntl/tonnoaM:taa ieigoioliieveenil@o(aias leyid\n",
      "batch #:1300   loss: 0.095554880797863 \n",
      "Primed with X:\n",
      "X:0:e/lc2 ed F/E | A cBc | B2| d2Bi dc | G2 | B2 | d2 G2A | d2 efP ea  atomeo.atoe:4t5\n",
      ":do/os\n",
      "\n",
      "::o: |\n",
      "Primed with <:\n",
      "<ddte |B\n",
      "GB | G2 | BBB2 FB\n",
      "] | c3 BB |// | |4/2 2d | d2| f | | BdB2B | | e/ e> f0 | B2 | Bd | e=cc | \n",
      "batch #:1400   loss: 0.08712729066610336 \n",
      "Primed with X:\n",
      "X fe |4>2]\n",
      " B2 | B2e | e4d2d/)dd | FB2/>B2 | F>d>c. |e | B2 B/ | |d/fc | cf | (|f2FB| B2 |  B | |ABcB\n",
      "Primed with <:\n",
      "<| |tc>| fB c | BBA c >/ M| | A>c>c c c | \n",
      "|fd |>d d2| F>d\n",
      "B/ ddB/B | B >2d) c>B/>>)\n",
      "4:3f>d2 | BA | B\n",
      "batch #:1500   loss: 0.06942863762378693 \n",
      "Primed with X:\n",
      "X2 >f | fB/ef z | c/ A Bo decg o2 B/ABc  | BBc | df | Ffe/ | f2 GG | G3/2 Gat/BBAB dc | BB | BB B/ | \n",
      "Primed with <:\n",
      "<e\n",
      "eB c  | F2| A|BBABA | B2d| | | G2A>Bc d2 | :e/0/ f2\n",
      "> B/ B2 Bc | B4  | cGe G.| Fd| AB | c/ BBA/ A2\n",
      "batch #:1600   loss: 0.07696786522865295 \n",
      "Primed with X:\n",
      "X\n",
      "O:sroti(laMM:TrarnM\n",
      "S:Pra?reuuvaoLO< cn\n",
      "::80>ne\n",
      "(:C/raVdd\n",
      "S:Trorae dal\n",
      "s:Tanoutounce\n",
      " ::ovlte\n",
      "R:\n",
      "fr\n",
      "Primed with <:\n",
      "<Cn|\n",
      "Mgansere\n",
      "M::artreve cum8e\n",
      "O:Cannoerse (E\n",
      "8:2euln\n",
      ":arrloureet \n",
      "R:5atomec ga\n",
      "s:Serve d.\n",
      "K:Ocaa@>(c\n",
      "batch #:1700   loss: 0.06615162640810013 \n",
      "Primed with X:\n",
      "X\n",
      "<er| f2crced | e)| c2d e c d c c|f3\n",
      "cf fee|d2d2f3c e2d gAB| f2e|f2 ||\n",
      ":G G2] d2d|c3 | e |de.A/fB |/\n",
      "Primed with <:\n",
      "< | c2 dcd c2edf| f2c d2d | B2c |c3 F3 |bf3|\n",
      "33 =2d>d d3f|d2e2 F3A d/e| cdB fde [2e|d2f2 | d c f | c3\n",
      "batch #:1800   loss: 0.08829706907272339 \n",
      "Primed with X:\n",
      "X: >h fige\n",
      "W:.antie, :- Z:Fn?an0tan !atove? :Traneenretc\n",
      "O:T:b9 :Larg? ma:qra>jo:\n",
      "cT0and/g\n",
      "O:Fa | d2A\n",
      "Primed with <:\n",
      "<c e d B d d f 2 f f2f d c de M c2^A G | .!G| Q2 - F G B2c fzB | F2=AG A2A | B2 | 32B |c22 | c2A| B !\n",
      "batch #:1900   loss: 0.0726667195558548 \n",
      "Primed with X:\n",
      "X:F- d> | B2c | | A2 | d2 | | Fz | !d2 c d| | cz- A/ | 3- c c B | c2d | F2 A | Fd | !f3 | c3 | c A | \n",
      "Primed with <:\n",
      "<] G2A | BA A| A | | ddd | f3 | c2 G e d2 | B/ B c | cB | f2 z2 | c2 | B2 | d f | fd | f2B A d | B2 G\n",
      "batch #:2000   loss: 0.08130408823490143 \n",
      "Primed with X:\n",
      "X\n",
      "LN8\"e903\n",
      "C:L!r8\n",
      "T:Fartn!cr\n",
      "O:F/o4mfc/2F3 d2e//2/ |/2(d2g||3e//3/2B2 [2nc22|22z/2s42-af2 (|\n",
      "c6/2 e3M\n",
      "Primed with <:\n",
      "<2f22d2 |c2/2 3| F[f3-2G| (3/22 | B22c2| G3/2A///23/ed//e/F/ f2 |z2=2f2 c3//2//2A8\n",
      "X_Fabd2e2 |Ge/2f d\n",
      "batch #:2100   loss: 0.05146186798810959 \n",
      "Primed with X:\n",
      "XB f\n",
      "\n",
      "<Mc>c2 | B3eBB | B2BA | B2B|| |e/B2 | B2BeB | B4B | d3 | B2e4 B2B | c3B | | B2d2 | B4B | B>B | \n",
      "Primed with <:\n",
      "<3 B | A2e2| (3G2 | (2B2 | B2BBB B | B3B>B | | G2c | BB BB4 | B2B/B B | G4 | B2B3c | B32B | ce | d4 B\n",
      "batch #:2200   loss: 0.06091773137450218 \n",
      "Primed with X:\n",
      "Xouh { dB | (B4 | c/B !2 c>e |>2 B2 c2 | BB/B2 | Bdd B2 d2 B> | c/ | c2 |B4 | c2F2 | dbordt tt2AA | B\n",
      "Primed with <:\n",
      "<ralhta!th |f/B BB B>\n",
      "CFch | erasirathMicas d>rinM>?/2\n",
      "K:F/8\n",
      "K:C\n",
      "\n",
      "LO1)f2o2/B2 B2 | A2 | /2 zB>B c e |\n",
      "batch #:2300   loss: 0.0766407698392868 \n",
      "Primed with X:\n",
      "Xs\n",
      "<et |\n",
      ":B3\n",
      "B:  c d cd | A2B c2d | B2 dc | g2 | d2/ c B | f>f cF2 MiG> | 422 | :\n",
      "gzB | c2c c2B | c3c\n",
      "Primed with <:\n",
      "<-z4 BA B2 | BLd d2 | A e cAc | [3) | Bc B2 d>B A | | c2 c | :2A A | A[F:c c g3 e2 | d2d dBz | d z g2\n",
      "batch #:2400   loss: 0.07642466574907303 \n",
      "Primed with X:\n",
      "X| A2d | f>daf.c2A2|A32g2Adc3 2d | F3AG2/2/A/c2 22B4-f2AA2 | [gif|\n",
      "<frthgat f2A2|\n",
      "I>A | 2f2BF2| d2B/2\n",
      "Primed with <:\n",
      "</4f/Gb/2d4 G2A(>///B^ || fA8/2Az_|g2c2/B3 | f2 A2z4| 2B3 F2F2c2c2 f2|| f2f2|\n",
      "<enthM:]\n",
      "1:8\n",
      "K:2anMB/B4\n",
      "batch #:2500   loss: 0.08241821080446243 \n",
      "Primed with X:\n",
      "X2dG (d2zB | d2d>c B>d>.B2B4 d2 d>f f>c Bc>cfdd A2BBB | B2A A>d A2B B2B2ABc d>e | c4BA>f | c2BB A>B F\n",
      "Primed with <:\n",
      "<>e d>E A2B2 B2A2B2d |BABAA d2cdd |E32cgbcef | d2d2dc B>dAc A2A/4 | B3d2A2BG f2 | (f4c>F A2A>d B2d>f \n",
      "batch #:2600   loss: 0.05426383763551712 \n",
      "Primed with X:\n",
      "X\n",
      "O:Trance\n",
      "R:Carchet er Mtch- 2ouLt1\n",
      "Z:yr2\n",
      "S:Pronen gourin par iin?or-1 B2 d2 | BFAeF/B/ | d2c2d | | \n",
      "Primed with <:\n",
      "<e cez | | 2c4 ! dc | d2e2 | d2z2e | g2B>d | c2B c2B>f | d2d>f | d>d | B4dcc | | BBB2BB | e2ded | c2e\n",
      "batch #:2700   loss: 0.07384723424911499 \n",
      "Primed with X:\n",
      "Xfd | c3d>c f3 BBB | f2B2 c4c2cF2 | f2f> | dBB | d2 | d>e | BBAA>B B2B>d | d3 | fde | f2c>c cz2 | B2c\n",
      "Primed with <:\n",
      "<fedc | dde | (d>c | cc/c>d  B>d> | B2c4 | B2BA | d2z >b | f/ |f2dB>c c>d | c>d c>3| B>A | BBB BB Bc \n",
      "batch #:2800   loss: 0.05351046845316887 \n",
      "Primed with X:\n",
      "X0056\n",
      "S:Pourtouil?\n",
      "C:Canch\n",
      "X:ProN bourilt @orien Maron eran Grin :urvanel \n",
      "a\n",
      "M:C\n",
      "Z:FranMc er gaourovi\n",
      "Primed with <:\n",
      "<nataf? ganevce\n",
      "Z:Can eourint? porve gs paran gamer (ilalcod dieW (8\n",
      "T:Trance\n",
      "L:Crvvencovie@\n",
      ":Mari\n",
      "T:\n",
      "batch #:2900   loss: 0.06851091235876083 \n",
      "Primed with X:\n",
      "X>d g eI | ff | d2 B2 | e2 | f2 | e2 dc | f2 e3 | (fe | d4 | G2 | B2 c  | d=B2 | c2 dB2 | B2 B2 | d2f\n",
      "Primed with <:\n",
      "</e | B3 d2 | Pd | c2g2 | t6 | f2 | B2 BB | B2 cd | \n",
      "6 | B2 eB  | B2 | B2c (| B2  A z2 | g6 | d2 d2 A\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 1\n",
    "plot_every = 10\n",
    "input_dim = 93\n",
    "output_dim = 93\n",
    "hidden_dim = 100\n",
    "# n_layers = 1\n",
    "lr = 0.001\n",
    "verbose = False\n",
    "\n",
    "# Create an instance of the LSTM and set optimizer and loss function\n",
    "model = LSTM_ABC(input_dim, hidden_dim, output_dim).type(gpu_dtype)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# The output of the model is softmax log prob, so the loss function is \n",
    "# NLL (negative log likelihood)\n",
    "loss_function = nn.NLLLoss()   \n",
    "\n",
    "all_losses = []  # keep track of loss history\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # loss = train(*random_training_set())   \n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    for t, sentence in enumerate(loader_train):\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        hidden = model.init_hidden()\n",
    "       \n",
    "        # Step 2. Get our inputs and targets ready for the network\n",
    "        # input will be fed character by character into LSTM\n",
    "        inp = sentence[:-1,:]\n",
    "        if verbose:\n",
    "            print(display_batch(inp, char_dict)) \n",
    "        # LSTM output will be compared against the target, which is \n",
    "        target = sentence[1:,:].type(torch.LongTensor)  \n",
    "        if verbose:\n",
    "            print(display_batch(target, char_dict))\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        target = target.cuda()\n",
    "        scores = model(Variable(inp.type(gpu_dtype)))\n",
    "        if verbose:\n",
    "            print (scores.size())\n",
    "            print (target.size())\n",
    "            \n",
    "       # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        target_index = torch.max(target,1)[1]  # NLLoss accept index as target, not one-hot\n",
    "        loss = loss_function(scores, Variable(target_index))\n",
    "        loss.backward(retain_graph=True)\n",
    "        # torch.nn.utils.clip_grad_norm(model.parameters(), 50)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if t%100 == 0:\n",
    "            print (\"batch #:{}   loss: {} \".format(t, loss.data[0]/len(sentence)))\n",
    "            predicted = evaluate(prime_str='X')\n",
    "            print(\"Primed with X:\")\n",
    "            print(predicted)\n",
    "            predicted = evaluate(prime_str='<')\n",
    "            print(\"Primed with <:\")\n",
    "            print(predicted) \n",
    "            all_losses.append(loss.data[0]/len(sentence))\n",
    "    \n",
    " \n",
    "    if epoch % print_every == 0:\n",
    "        print('[(%d %d%%) %.4f]' % (epoch, epoch / n_epochs * 100, loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:116\n",
      "L:1/8\n",
      "O:1\n",
      "R:3e\"stoderit on coutielsove?.er tonceltames pari? sarnscitem(itomte paisle 12e cELLO\n"
     ]
    }
   ],
   "source": [
    "predicted = evaluate()\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff5c5301b70>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nt7XxGCzpx5EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff62f3e24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
